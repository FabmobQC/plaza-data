{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from shapely.geometry import Polygon\n",
    "from ydata_profiling import ProfileReport\n",
    "import json\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "def coord_lister(geom):\n",
    "    return (json.loads(geom)['features'][0]['geometry']['coordinates'])\n",
    "\n",
    "def part_of_journey(hour):\n",
    "  if hour <= datetime.time(6, 30):\n",
    "      return '1 - Avant 6h30'\n",
    "  if datetime.time(6, 30) < hour <= datetime.time(9, 30):\n",
    "      return '2 - AM (6h30 à 9h30)'\n",
    "  if datetime.time(9, 30) < hour <= datetime.time(15, 30):\n",
    "      return '3 - Jour (9h30 à 15h30)'\n",
    "  if datetime.time(15, 30) < hour <= datetime.time(18, 30):\n",
    "      return '4 - PM (15h30 à 18h30)'\n",
    "  if datetime.time(18, 30) < hour :\n",
    "    return '5 - Soir (après 18h30)'\n",
    "  else:\n",
    "    return 'ERROR'\n",
    "\n",
    "def part_of_journey_stm(period):\n",
    "  if period == 'Avant 6h30':\n",
    "      return '1 - Avant 6h30'\n",
    "  if period == 'AM (6h30 à 9h30)':\n",
    "      return '2 - AM (6h30 à 9h30)'\n",
    "  if period == 'Jour (9h30 à 15h30)':\n",
    "      return '3 - Jour (9h30 à 15h30)'\n",
    "  if period == 'PM (15h30 à 18h30)':\n",
    "      return '4 - PM (15h30 à 18h30)'\n",
    "  if period == 'Soir (après 18h30)':\n",
    "    return '5 - Soir (après 18h30)'\n",
    "  else:\n",
    "    return 'ERROR'\n",
    "\n",
    "\n",
    "def generate_metatdaa(df, decription, name):\n",
    "    profile = ProfileReport(\n",
    "        df,\n",
    "        title=\"Metadata \"+name,\n",
    "        missing_diagrams=None,\n",
    "        correlations=None,\n",
    "        interactions=None,\n",
    "        duplicates=None,\n",
    "        vars={\n",
    "            \"cat\": {\n",
    "                \"length\": True,\n",
    "                \"characters\": False,\n",
    "                \"words\": False,\n",
    "            }\n",
    "            }\n",
    "        )\n",
    "    profile.config.variables.descriptions = decription\n",
    "    profile.to_file(\"metadata/metadata_\"+name+\".html\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation du polygon pour la plaza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_rosemont_boyer= [45.53414890262374, -73.59569524098734]\n",
    "coin_rosemont_st_vallier = [45.53181801496085, -73.59784906811936]\n",
    "coin_jean_talon_st_vallier = [45.53915366061581, -73.61386735169552]\n",
    "coin_jean_talon_boyer = [45.54174620305895, -73.61260342943066]\n",
    "\n",
    "lat_point_list = [45.53414890262374, 45.53181801496085, 45.53915366061581, 45.54174620305895, 45.53414890262374]\n",
    "lon_point_list = [-73.59569524098734, -73.59784906811936, -73.61386735169552, -73.61260342943066, -73.59569524098734]\n",
    "\n",
    "polygon_geom = Polygon(zip(lon_point_list, lat_point_list))\n",
    "crs = 'epsg:4326'\n",
    "polygon_plaza = gpd.GeoDataFrame( crs=crs, geometry=[polygon_geom])  \n",
    "\n",
    "\n",
    "# map = folium.Map([45.53619845067009, -73.60313339047141], zoom_start=15, tiles='cartodbpositron')\n",
    "# folium.GeoJson(polygon_plaza).add_to(m)\n",
    "# folium.LatLngPopup().add_to(m)\n",
    "# map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarize dataset: 100%|██████████| 23/23 [00:00<00:00, 149.16it/s, Completed]                              \n",
      "Generate report structure: 100%|██████████| 1/1 [00:02<00:00,  2.30s/it]\n",
      "Render HTML: 100%|██████████| 1/1 [00:00<00:00,  5.56it/s]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 492.17it/s]\n",
      "Summarize dataset: 100%|██████████| 18/18 [00:45<00:00,  2.51s/it, Completed]                            \n",
      "Generate report structure: 100%|██████████| 1/1 [00:01<00:00,  1.68s/it]\n",
      "Render HTML: 100%|██████████| 1/1 [00:00<00:00,  7.20it/s]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 497.25it/s]\n",
      "Summarize dataset: 100%|██████████| 15/15 [00:00<00:00, 358.19it/s, Completed]                              \n",
      "Generate report structure: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n",
      "Render HTML: 100%|██████████| 1/1 [00:00<00:00, 12.89it/s]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 1358.70it/s]\n",
      "Summarize dataset: 100%|██████████| 36/36 [00:00<00:00, 302.30it/s, Completed]                    \n",
      "Generate report structure: 100%|██████████| 1/1 [00:03<00:00,  3.35s/it]\n",
      "Render HTML: 100%|██████████| 1/1 [00:00<00:00,  3.83it/s]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 820.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# COMMUNAUTO\n",
    "communauto_df = pd.read_excel('data/Communauto/Données brutes Communauto.xlsx')\n",
    "communauto_gdf = gpd.GeoDataFrame(communauto_df, geometry=gpd.points_from_xy(communauto_df.ArretLongitude, communauto_df.ArretLatitude))\n",
    "communauto_gdf.crs = 'epsg:4326'\n",
    "\n",
    "# extract metadata\n",
    "columns = {column: column for column in communauto_df.columns.to_list()}\n",
    "name = 'communauto'\n",
    "generate_metatdaa(communauto_df, columns, name)\n",
    "\n",
    "\n",
    "# BIXI\n",
    "bixi_station_df = pd.read_csv('data/Bixi/2021_stations.csv')\n",
    "bixi_station_gdf = gpd.GeoDataFrame(bixi_station_df, geometry=gpd.points_from_xy(bixi_station_df.longitude, bixi_station_df.latitude))\n",
    "bixi_station_gdf.crs = 'epsg:4326'\n",
    "\n",
    "bixi_data_df = pd.read_csv('data/Bixi/2021_donnees_ouvertes.csv')\n",
    "\n",
    "bixi_merged_df_1 = pd.merge(bixi_data_df,bixi_station_gdf, \n",
    "                     left_on='emplacement_pk_start',\n",
    "                     right_on=bixi_station_gdf['pk'],\n",
    "                     how='left')\n",
    "\n",
    "bixi_merged_df_1 = bixi_merged_df_1.fillna(0)\n",
    "bixi_merged_df = pd.merge(bixi_merged_df_1, bixi_station_gdf,\n",
    "                     left_on='emplacement_pk_end',\n",
    "                     right_on=bixi_station_gdf['pk'],\n",
    "                     how='left',\n",
    "                      suffixes=('_start', '_end'))\n",
    "bixi_merged_df.drop(columns=['latitude_start', 'longitude_start','latitude_end', 'longitude_end'], inplace = True)\n",
    "bixi_merged_gdf = gpd.GeoDataFrame(bixi_merged_df)\n",
    "\n",
    "# extract metadata\n",
    "columns_bixi =  {\n",
    "    \"start_date\": \"Date et heure de début du déplacement au formant AAAA-MM-JJ hh:mm\",\n",
    "    \"emplacement_pk_start\": \"Identifiant de la station du début du trajet\",\n",
    "    \"end_date\": \"Date et heure de la fin du déplacement au formant AAAA-MM-JJ hh:mm\",\n",
    "    \"emplacement_pk_end\": \"Identifiant de la station de fin du trajet\",\n",
    "    \"duration_sec\": \"Durée total du déplacement en secondes\",\n",
    "    \"is_member\": \"Type d'utilisateur. Valeurs possibles : 1 : Abonné du réseau BIXI Montréal,0 : Non-abonné du réseau BIXI Montréal\",\n",
    "    'pk_start': 'id de la station de depart', \n",
    "    'name_start': 'nom de la station de depart', \n",
    "    'geometry_start': 'geolcalisation de la station de depart', \n",
    "    'pk_end':'id de la station de destination', \n",
    "    'name_end': 'nom de la station de destination' , \n",
    "    'geometry_end': 'geolcalisation de la station de destination' , \n",
    "}\n",
    "name = 'Bixi'\n",
    "generate_metatdaa(bixi_merged_df, columns_bixi, name)\n",
    "\n",
    "\n",
    "# display(bixi_merged_gdf)\n",
    "\n",
    "# STM\n",
    "stm_df = pd.read_excel('data/STM/Résultat_Projet FabMob Plaza St-H - Liste des arrêts.xlsx', sheet_name='Données')\n",
    "stm_gdf = gpd.GeoDataFrame(stm_df, geometry=gpd.points_from_xy(stm_df.X, stm_df.Y))\n",
    "stm_gdf.crs = 'epsg:2950'\n",
    "stm_gdf = stm_gdf.to_crs(4326)\n",
    "\n",
    "\n",
    "# extract metadata\n",
    "columns = {column: column for column in stm_df.columns.to_list()}\n",
    "name = 'STM'\n",
    "generate_metatdaa(stm_df, columns, name)\n",
    "\n",
    "\n",
    "# AMD\n",
    "amd_df = pd.read_csv('data/AMD/Taux d\\'occupation par troncon - demande du 2022-10-27 - Plaza St-Hubert - v1.csv')\n",
    "from shapely import wkt\n",
    "amd_df['geometry'] = amd_df['geometry'].apply(wkt.loads)\n",
    "amd_gdf = gpd.GeoDataFrame(amd_df, geometry=amd_df.geometry)\n",
    "\n",
    "amd_gdf.crs = 'epsg:4326'\n",
    "\n",
    "# extract metadata\n",
    "columns = {column: column for column in amd_df.columns.to_list()}\n",
    "name = 'AMD'\n",
    "generate_metatdaa(amd_df, columns, name)\n",
    "\n",
    "\n",
    "# display(amd_gdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Selection des données pour l'experimentation\n",
    "#### Bixi: \n",
    "##### - Selection des données du 1er au 31 octobre 2021\n",
    "##### - Prise en compte des points d'arrivee seulement\n",
    "##### - Gestion des periodes de la journée du stop\n",
    "\n",
    "#### Communauto:\n",
    "##### - Selection des trajets qui font un arret dans la quartier de la plaza: Type d'arret = Arret ou Nom de station = Flex Montréal\n",
    "##### - prise en compte des dates et du point de géolocalisation de TripEventDate\n",
    "##### - Gestion des periodes de la journée du stop\n",
    "\n",
    "### MTL Trajet\n",
    "##### Les données n'ont pas ete integrées car elles sont aggrégées pôur H2S donc trop large pour notre cas d'usage\n",
    "\n",
    "### Parco\n",
    "##### - les taux d'occupation ont ete regroupée en période de la journée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def row_to_json(cell):\n",
    "    # display(list(cell.coords))\n",
    "    # print(cell)\n",
    "    return np.array(np.array(cell.coords))\n",
    "    # return cell.to_json()\n",
    "\n",
    "def route_to_feature(line):\n",
    "    data = {\n",
    "        \"type\": \"Feature\",\n",
    "        \"geometry\": \n",
    "            {\n",
    "            \"type\": line.type,\n",
    "            \"coordinates\":np.array(np.array(line.coords)).tolist()\n",
    "            }\n",
    "    }\n",
    "    return json.dumps(data)\n",
    "\n",
    "amd_gdf_atraiter = amd_gdf.copy()\n",
    "cols = ['Rue','Entre la rue',\t'Et la rue','Du coté']\n",
    "amd_gdf_atraiter['nom_station'] = amd_gdf_atraiter['Rue']\n",
    "amd_gdf_atraiter['Entre la rue'].loc[amd_gdf_atraiter['Entre la rue'].str.contains('Terrain', case=False)] = ''\n",
    "amd_gdf_atraiter['Et la rue'].loc[amd_gdf_atraiter['Et la rue'].str.contains('Terrain', case=False)] = ''\n",
    "amd_gdf_atraiter['Du coté'].loc[amd_gdf_atraiter['Du coté'].str.contains('Terrain', case=False)] = ''\n",
    "\n",
    "amd_gdf_atraiter['nom_station'] = amd_gdf_atraiter[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "amd_gdf_atraiter['1 - Avant 6h30'] = amd_gdf_atraiter[['Hr00', 'Hr01',  'Hr02',  'Hr03',  'Hr04',  'Hr05',  'Hr06',  'Hr07']].mean(axis=1)\n",
    "amd_gdf_atraiter['2 - AM (6h30 à 9h30)'] = amd_gdf_atraiter[['Hr06', 'Hr07', 'Hr08',  'Hr09',  'Hr10']].mean(axis=1)\n",
    "amd_gdf_atraiter['3 - Jour (9h30 à 15h30)'] = amd_gdf_atraiter[['Hr09','Hr10', 'Hr11',  'Hr12',  'Hr13',  'Hr14', 'Hr15',  'Hr16']].mean(axis=1)\n",
    "amd_gdf_atraiter['4 - PM (15h30 à 18h30)'] = amd_gdf_atraiter[['Hr15',  'Hr16', 'Hr17',  'Hr18',  'Hr19']].mean(axis=1)\n",
    "amd_gdf_atraiter['5 - Soir (après 18h30)'] = amd_gdf_atraiter[['Hr18',  'Hr19', 'Hr20',  'Hr21',  'Hr22',  'Hr23']].mean(axis=1)\n",
    "amd_gdf_atraiter.drop(columns=['Rue', 'Entre la rue','Et la rue','Du coté','Hr00','Hr01','Hr02','Hr03','Hr04','Hr05','Hr06','Hr07','Hr08','Hr09','Hr10','Hr11','Hr12','Hr13','Hr14','Hr15','Hr16','Hr17','Hr18','Hr19','Hr20','Hr21','Hr22','Hr23'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "amd_gdf_atraiter_2 = amd_gdf_atraiter.melt(id_vars=[\"No_Troncon\",'geometry','nom_station'], var_name=\"periode\", value_name=\"taux_occupation\")\n",
    "amd_gdf_atraiter_2['IsWeekend'] = 'False'\n",
    "amd_gdf_atraiter_2['mode'] = 'Stationnement'\n",
    "amd_gdf_atraiter_2['Nb_personnes']= np.NaN\n",
    "# amd_gdf_atraiter_2['geom_json']= amd_gdf_atraiter_2['geometry'].to_json()\n",
    "amd_gdf_atraiter_2['geom_json']= amd_gdf_atraiter_2['geometry'].apply(row_to_json)\n",
    "amd_gdf_atraiter_2['geom_json_2']= amd_gdf_atraiter_2['geometry'].apply(route_to_feature)\n",
    "\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.max_colwidth',1000):  # more options can be specified also\n",
    "#     display(amd_gdf_atraiter_2['geom_json'].head(20))\n",
    "# amd_gdf_atraiter_2['geom_array'] = amd_gdf_atraiter_2.geom_json.apply(coord_lister)\n",
    "amd_gdf_atraiter_2 = amd_gdf_atraiter_2.rename(columns={'No_Troncon': ''})\n",
    "# amd_gdf_atraiter_2.drop(columns=['geom_json'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "display(amd_gdf_atraiter_2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communauto_gdf['datDateDebutReservation'] = pd.to_datetime(communauto_gdf['datDateDebutReservation'], format='%d/%m/%y %H:%M')\n",
    "communauto_gdf['datDateFinReservation'] = pd.to_datetime(communauto_gdf['datDateFinReservation'], format='%d/%m/%y %H:%M')\n",
    "communauto_gdf['TripEventDate'] = pd.to_datetime(communauto_gdf['TripEventDate'], format='%d/%m/%y %H:%M')\n",
    "bixi_merged_gdf['end_date'] = pd.to_datetime(bixi_merged_gdf['end_date'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "\n",
    "bixi_extract = bixi_merged_gdf[(bixi_merged_gdf['end_date']<= '2021-10-31')& (bixi_merged_gdf['end_date']>= '2021-10-01')].copy()\n",
    "communauto_gdf['periode'] = communauto_gdf.apply(lambda trip: part_of_journey(trip[\"TripEventDate\"].time()),axis=1)\n",
    "bixi_extract['periode'] = bixi_extract.apply(lambda trip: part_of_journey(trip[\"end_date\"].time()),axis=1)\n",
    "stm_gdf['Période'] = stm_gdf.apply(lambda trip: part_of_journey_stm(trip[\"Période\"]),axis=1)\n",
    "\n",
    "\n",
    "\n",
    "bixi_extract.set_geometry(\"geometry_end\", inplace=True)\n",
    "bixi_extract_2 = bixi_extract.sjoin(polygon_plaza, how=\"inner\", predicate='within')\n",
    "display(bixi_extract_2)\n",
    "\n",
    "bixi_data = bixi_extract_2[['periode', 'end_date', 'emplacement_pk_end', 'name_end','is_member', 'geometry_end']].copy()\n",
    "bixi_data[\"type d'arrêt\"] = 'Fin de trajet'\t\n",
    "bixi_data[\"mode\"] = 'Bixi'\t\n",
    "# bixi_data.drop(columns='duration_sec', inplace=True)\n",
    "bixi_data = bixi_data.rename({'end_date': 'date_stop', 'emplacement_pk_end': 'id_station', 'name_end': 'nom_station', \"Type d'arrêt\": \"type_arret\", \"geometry_end\": 'geometry'}, axis=1)\n",
    "\n",
    "communauto_extract= communauto_gdf[(communauto_gdf[\"Type d'arrêt\"]=='Arrêt') | (communauto_gdf[\"strNomStation\"]=='FLEX Montréal')].copy()\n",
    "communauto_data = communauto_extract[['periode', 'TripEventDate', 'StationNo', 'strNomStation','Type d\\'arrêt','geometry']].copy()\n",
    "\n",
    "communauto_data['is_member'] =1\n",
    "communauto_data[\"mode\"] = 'Communauto'\t\n",
    "communauto_data.reset_index(inplace=True)\n",
    "# communauto_data.drop(columns='TripEventDate_idx', inplace=True)\n",
    "communauto_data = communauto_data.rename({'TripEventDate': 'date_stop', 'StationNo': 'id_station', 'strNomStation': 'nom_station', \"Type d'arrêt\": \"type_arret\"}, axis=1)\n",
    "\n",
    "# rdf = gpd.GeoDataFrame( pd.concat( dataframesList, ignore_index=True) )\n",
    "\n",
    "data_plaza =gpd.GeoDataFrame( pd.concat([bixi_data, communauto_data], ignore_index=True))\n",
    "data_plaza['day_of_week'] = data_plaza['date_stop'].dt.day_name()\n",
    "data_plaza['IsWeekend'] = data_plaza['date_stop'].dt.weekday >= 5\n",
    "\n",
    "data_plaza['date'] = data_plaza['date_stop'].dt.date\n",
    "display(data_plaza)\n",
    "data_plaza_json =data_plaza.copy()\n",
    "data_plaza_json['date_stop'] = data_plaza_json['date_stop'].astype(str)\n",
    "data_plaza_json['date'] = data_plaza_json['date'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plaza_json.to_file(\"test.geojson\", driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plaza_agregat= data_plaza.groupby(['date','periode', 'nom_station','mode']).agg({'id_station':'count', 'IsWeekend': 'first', 'geometry': 'first'}).reset_index()\n",
    "display(data_plaza_agregat)\n",
    "data_plaza_agregat.to_excel(\"data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stm_gdf_ready = stm_gdf[['Période', 'Description de l\\'arrêt', 'Descendants', 'geometry']].copy()\n",
    "stm_gdf_ready['IsWeekend']= 'False'\n",
    "stm_gdf_ready['mode']= 'Bus'\n",
    "stm_gdf_ready['Descendants'] = stm_gdf_ready['Descendants']*-1\n",
    "stm_gdf_ready = stm_gdf_ready.rename({'Période': 'periode','Description de l\\'arrêt': 'nom_station','Descendants':'Nb_personnes'}, axis=1)\n",
    "# display(stm_gdf_ready)\n",
    "\n",
    "data_plaza_agregat_all = data_plaza_agregat.groupby(['IsWeekend','periode', 'nom_station','mode']).agg({'id_station':'mean', 'geometry': 'first'}).reset_index()\n",
    "data_plaza_agregat_all = data_plaza_agregat_all.rename({'id_station': 'Nb_personnes'}, axis=1)\n",
    "data_plaza_agregat_all.to_excel(\"data_agregat_all.xlsx\")\n",
    "\n",
    "data_plaza_agregat_week = data_plaza_agregat_all[data_plaza_agregat_all['IsWeekend']==False]\n",
    "# display(data_plaza_agregat_week)\n",
    "\n",
    "data_plaza_agregat_week_gdf = gpd.GeoDataFrame(data_plaza_agregat_week, geometry=data_plaza_agregat_week['geometry'])\n",
    "data_plaza_agregat_week_gdf =gpd.GeoDataFrame( pd.concat([data_plaza_agregat_week_gdf, stm_gdf_ready], ignore_index=True))\n",
    "\n",
    "data_plaza_agregat_week_gdf['lon'] = data_plaza_agregat_week_gdf['geometry'].x\n",
    "data_plaza_agregat_week_gdf['lat'] = data_plaza_agregat_week_gdf['geometry'].y\n",
    "\n",
    "data_plaza_agregat_week_gdf = pd.concat([data_plaza_agregat_week_gdf, amd_gdf_atraiter_2])\n",
    "\n",
    "display(data_plaza_agregat_week_gdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "data_plaza_agregat_week_gdf.to_file(\"data_plaza_agregat_week.geojson\", driver='GeoJSON')\n",
    "data_plaza_agregat_week_gdf[data_plaza_agregat_week_gdf['mode']=='Stationnement'].to_file(\"data_plaza_agregat_week_AMD.geojson\", driver='GeoJSON')\n",
    "data_plaza_agregat_week_gdf[data_plaza_agregat_week_gdf['mode']=='Stationnement'].to_csv(\"data_plaza_agregat_week_AMD.csv\")\n",
    "data_plaza_agregat_week_gdf.to_csv(\"data_plaza_agregat_week.csv\") \n",
    "display(data_plaza_agregat_week_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
